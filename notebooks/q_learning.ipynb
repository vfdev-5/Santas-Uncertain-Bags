{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning tryouts on Santa's uncertain bags\n",
    "\n",
    "## States \n",
    "\n",
    "A state is characterized by a matrix of size `(N_BAGS, N_TYPES)`. For example, `s[0,:]=[1,0,1,0,0,0,0,0,0]`. The initial state is when the matrix is null or a customly defined. Terminal states are defined by state's score. \n",
    "\n",
    "How many state there are? There are at most `N_BAGS * 9^10` states.\n",
    "\n",
    "\n",
    "## Actions\n",
    "\n",
    "Action is to add a toy following the list of available toys.\n",
    "\n",
    "\n",
    "## Rewards\n",
    "\n",
    "Action reward can be defined by the score of the bag where a toy has been added.\n",
    "\n",
    "\n",
    "## Q-learning: Off-Policy Temporal Difference Control\n",
    "\n",
    "In this algorithm we estimate action-value function $Q(s,a)$ as :\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t,A_t) \\right], \\, Q(\\cal{S}^{+},a)=0\n",
    "$$\n",
    "\n",
    "**Algorithm**\n",
    "<br>\n",
    "<div style=\"background-color: #aaaaaa; padding: 10px; width: 75%; border: solid black; border-radius: 5px;\">\n",
    "\n",
    "    Initialize $Q(s, a)$, for all $s \\in \\cal{S}$, $a \\in \\cal{A}(s)$, arbitrarily, and $Q(\\text{terminal-state}, \\cdot) = 0$<br>\n",
    "    Repeat (for each episode):<br>\n",
    "    &emsp;Initialize $S$<br>\n",
    "    &emsp;Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)<br>\n",
    "    &emsp;Repeat (for each step of episode):<br>\n",
    "    &emsp;&emsp;Take action $A$, observe $R$, $S'$<br>\n",
    "    &emsp;&emsp;$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\max_{a}Q(S', a) - Q(S,A) \\right]$<br>\n",
    "    &emsp;&emsp;$S \\leftarrow S'; \\, A \\leftarrow A';$<br>\n",
    "    &emsp;until $S$ is terminal\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from utils import weight3 as weight_fn, weight_by_index\n",
    "from utils import bag_weight, score\n",
    "from utils import MAX_WEIGHT, AVAILABLE_GIFTS, GIFT_TYPES, N_TYPES, N_BAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000.0\n"
     ]
    }
   ],
   "source": [
    "initial_state = np.zeros((N_BAGS, N_TYPES), dtype=np.uint8)\n",
    "alpha = 0.72\n",
    "goal_weight = MAX_WEIGHT * N_BAGS * alpha\n",
    "\n",
    "print goal_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[0 0 0 ..., 0 0 0]\\n [0 0 0 ..., 0 0 0]\\n [0 0 0 ..., 0 0 0]\\n ..., \\n [0 0 0 ..., 0 0 0]\\n [0 0 0 ..., 0 0 0]\\n [0 0 0 ..., 0 0 0]]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def state_to_str(state):\n",
    "    return state.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inv_cdf(cdf, u):\n",
    "    out = 0\n",
    "    ll= len(cdf)\n",
    "    for i in range(1, ll):\n",
    "        if cdf[i-1] <= u < cdf[i]:\n",
    "            out = i\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def get_policy_action(state, policy_dict, return_index=False):\n",
    "    action_probas = policy_dict[state_to_str(state)]\n",
    "    ll = len(action_probas)\n",
    "    cdf = np.cumsum(action_probas)\n",
    "    if return_index:\n",
    "        index = inv_cdf(cdf, np.random.rand())\n",
    "        return ACTIONS[index], index\n",
    "    return ACTIONS[inv_cdf(cdf, np.random.rand())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(goal_weight, \n",
    "               available_gifts,\n",
    "               initial_state=None\n",
    "               n_episodes=10, alpha=0.75, gamma=0.7, epsilon=0.001, action_value_function=None):\n",
    "\n",
    "    policy_dict = defaultdict()\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        episode_length = 1000\n",
    "        state = np.zeros((N_BAGS, N_TYPES)) if initial_state is None else initial_state\n",
    "        \n",
    "        action = get_policy_action(state, policy_dict)\n",
    "        \n",
    "        state_score = score(state)        \n",
    "        while state_score > goal_weight:\n",
    "            \n",
    "            episode_length -= 1 \n",
    "            if episode_length < 0:\n",
    "                logging.warn('Episode length is reached, but state score is still : %f / %f' % (state_score, goal_weight))\n",
    "                break\n",
    "            \n",
    "            #print \"state, action : \", state, action\n",
    "            x, y = state\n",
    "            \n",
    "            current_reward = 0\n",
    "            nx, ny = take_action(state, action)\n",
    "            if not on_grid(nx, ny):\n",
    "                nx, ny = x, y\n",
    "                current_reward = REWARDS[2]\n",
    "            elif on_bridge(nx, ny) and not at_end_position(nx, ny):\n",
    "                current_reward = REWARDS[2]\n",
    "            elif in_pit(nx, ny):\n",
    "                current_reward = REWARDS[0]\n",
    "            elif at_end_position(nx, ny):\n",
    "                current_reward = REWARDS[1] \n",
    "                \n",
    "            new_state = (nx, ny)\n",
    "            new_action, new_action_index = get_policy_action(new_state, policy, return_index=True)\n",
    "            \n",
    "            # Update Q(s,a)\n",
    "            v = action_value_function[y, x, action_index]\n",
    "            nv = np.max(action_value_function[ny, nx, :])\n",
    "            t = alpha * (current_reward + gamma * nv - v) \n",
    "            action_value_function[y, x, action_index] += t\n",
    "            \n",
    "            # Update policy from Q(s,a) using epsilon-soft strategy\n",
    "            action_star_index = np.argmax(action_value_function[y, x, :])\n",
    "            for i in range(ll):\n",
    "                policy[y, x, i] = epsilon / ll\n",
    "            policy[y, x, action_star_index] = 1.0 - epsilon + epsilon / ll\n",
    "\n",
    "            state, action, action_index = new_state, new_action, new_action_index\n",
    "            \n",
    "            \n",
    "            state_score = score(state)\n",
    "                \n",
    "    return policy, action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
