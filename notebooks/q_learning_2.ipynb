{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Q-learning tryout on Santa's uncertain bags\n",
    "\n",
    "We reduce the number of possible states corresponding to 1000 bags to states corresponding to 1 bag. Problem of filling 1000 bags will be translated to the optimal usage of policy and action-value function on varying environment. The last is given by the array of available gifts which decreases when bags are filled.\n",
    "\n",
    "\n",
    "## States \n",
    "\n",
    "A state is characterized by a vector of size `(N_TYPES)`. For example, `s=[1,0,1,0,0,0,0,0,0]`. The initial state is when the null vector or a customly defined vector. Terminal states are defined by state's score. \n",
    "\n",
    "How many state there are? There are at most `10^N_TYPES` states.\n",
    "\n",
    "\n",
    "## Actions\n",
    "\n",
    "Action is to add a toy to the bag following the list of available toys. For example, action is a integer value corresponding to the toy index.\n",
    "\n",
    "\n",
    "## Rewards\n",
    "\n",
    "Action reward can be defined by the score of the bag where a toy has been added.\n",
    "\n",
    "\n",
    "## Q-learning: Off-Policy Temporal Difference Control\n",
    "\n",
    "In this algorithm we estimate action-value function $Q(s,a)$ as :\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t,A_t) \\right], \\, Q(\\cal{S}^{+},a)=0\n",
    "$$\n",
    "\n",
    "**Algorithm**\n",
    "<br>\n",
    "<div style=\"background-color: #aaaaaa; padding: 10px; width: 75%; border: solid black; border-radius: 5px;\">\n",
    "\n",
    "    Initialize $Q(s, a)$, for all $s \\in \\cal{S}$, $a \\in \\cal{A}(s)$, arbitrarily, and $Q(\\text{terminal-state}, \\cdot) = 0$<br>\n",
    "    Repeat (for each episode):<br>\n",
    "    &emsp;Initialize $S$<br>\n",
    "    &emsp;Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)<br>\n",
    "    &emsp;Repeat (for each step of episode):<br>\n",
    "    &emsp;&emsp;Take action $A$, observe $R$, $S'$<br>\n",
    "    &emsp;&emsp;$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\max_{a}Q(S', a) - Q(S,A) \\right]$<br>\n",
    "    &emsp;&emsp;$S \\leftarrow S'; \\, A \\leftarrow A';$<br>\n",
    "    &emsp;until $S$ is terminal\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(2017)\n",
    "\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from utils import weight3 as weight_fn, weight_by_index\n",
    "from utils import bag_weight, score, mean_n_sigma, score_stats\n",
    "from utils import MAX_WEIGHT, AVAILABLE_GIFTS, GIFT_TYPES, N_TYPES, N_BAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.015\n",
    "NEGATIVE_REWARD = -5000\n",
    "POSITIVE_REWARD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step_reward(rejected):    \n",
    "    return 1.0 if rejected < REJECTED_BAGS_THRESHOLD else -rejected*10\n",
    "\n",
    "def take_action(state, action):\n",
    "    new_state = state.copy()\n",
    "    new_state[action] += 1\n",
    "    return new_state\n",
    "\n",
    "def is_available(state, available_gifts, gift_types=GIFT_TYPES):\n",
    "    for v, gift_type in zip(state, gift_types):\n",
    "        if available_gifts[gift_type] - v < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def update_available_gifts(available_gifts, state, gift_types=GIFT_TYPES):\n",
    "    for v, gift_type in zip(state, gift_types):\n",
    "        assert available_gifts[gift_type] - v >= 0, \"Found state is not available : {}, {}\".format(state, available_gifts)\n",
    "        available_gifts[gift_type] = available_gifts[gift_type] - v\n",
    "        \n",
    "def state_to_str(state):\n",
    "    return state.tolist().__str__()\n",
    "\n",
    "def find_value(action, actions_values, return_index=False):\n",
    "    for i, (v, a) in enumerate(actions_values):\n",
    "        if action == a:\n",
    "            if return_index:\n",
    "                return v, i\n",
    "            return v\n",
    "    raise Exception(\"No action={} in actions_values={}\".format(action, actions_values))\n",
    "    \n",
    "def has_action(actions_values, action, return_index=False):\n",
    "    for i, (v, a) in enumerate(actions_values):\n",
    "        if action == a:\n",
    "            if return_index:\n",
    "                return True, i\n",
    "            return True\n",
    "    if return_index:\n",
    "        return False, None\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NULL_ACTIONS_VALUES = [(POSITIVE_REWARD, None)]\n",
    "\n",
    "def get_actions_values(state, action_value_function):\n",
    "    state_key = state_to_str(state)\n",
    "    actions_values = action_value_function[state_key]\n",
    "    if len(actions_values) == 0:\n",
    "        for i in range(N_TYPES):\n",
    "            va = [POSITIVE_REWARD - np.random.rand(), i]            \n",
    "            heapq.heappush(action_value_function[state_key], va)        \n",
    "    return action_value_function[state_key]    \n",
    "\n",
    "def set_actions_values(state, action_value_function, actions_values):\n",
    "    state_key = state_to_str(state)\n",
    "    action_value_function[state_key] = actions_values\n",
    "\n",
    "def get_policy_action(state, action_value_function, epsilon=0.1):\n",
    "    u = np.random.rand()\n",
    "    # Get max value action\n",
    "    actions_values = get_actions_values(state, action_value_function)\n",
    "    max_action_value = actions_values[0]\n",
    "    pr = 1.0 - epsilon + epsilon / N_TYPES\n",
    "    if u < pr:\n",
    "        # Greedy\n",
    "        return max_action_value[1]\n",
    "    else:\n",
    "        # Exploring\n",
    "        if max_action_value[1] is None:\n",
    "            return None\n",
    "        actions = list(range(N_TYPES))\n",
    "        actions.remove(max_action_value[1])\n",
    "        return actions[np.random.randint(N_TYPES-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_learning(goal_weight, \n",
    "               available_gifts,\n",
    "               initial_state=None,\n",
    "               n_episodes=10, alpha=0.75, gamma=0.95, epsilon=0.1, action_value_function=None):\n",
    "    \n",
    "    logging.info(\"--- Q-learning : goal={}, n_episodes={}\".format(goal_weight, n_episodes))\n",
    "    if action_value_function is None:\n",
    "        logging.info(\"-- Reset action_value_function\")\n",
    "        action_value_function = defaultdict(list)\n",
    "    \n",
    "    best_state = None\n",
    "    best_score = 0\n",
    "    \n",
    "    def _is_terminal_state(state, available_gifts, goal_weight):\n",
    "        _is_terminal = False\n",
    "        _current_reward = 0        \n",
    "        _state_score, _state_score_std, _rejected, _rejected_std = score_stats((state,), count=200)\n",
    "        _score_min = _state_score - _state_score_std*0.1\n",
    "        _score_max = _state_score + _state_score_std*0.5            \n",
    "        _rejected += _rejected_std*0.25\n",
    "        if not is_available(state, available_gifts) or _rejected > 2.0*REJECTED_BAGS_THRESHOLD:                \n",
    "            _current_reward = NEGATIVE_REWARD\n",
    "            _is_terminal = True\n",
    "            logging.debug(\"--->1 Episode finished with NEGATIVE reward, {}, {}, {}\".format(_score_min, _score_max, _rejected))                \n",
    "        elif _score_max >= MAX_WEIGHT:\n",
    "            _current_reward = NEGATIVE_REWARD\n",
    "            _is_terminal = True\n",
    "            logging.debug(\"--->2 Episode finished with NEGATIVE reward, {}, {}, {}\".format(_score_min, _score_max, _rejected))\n",
    "        elif MAX_WEIGHT > _score_min >= goal_weight:\n",
    "            _current_reward = POSITIVE_REWARD\n",
    "            _is_terminal = True\n",
    "            logging.debug(\"---> Episode finished with POSITIVE reward\")\n",
    "        elif _score_min < goal_weight:\n",
    "            _current_reward = step_reward(_rejected)\n",
    "        else:\n",
    "            raise Exception(\"Unclassified state: {}, score_min={}, score_max={}, rejected={}\".format(new_state, score_min, score_max, rejected))\n",
    "\n",
    "        return _is_terminal, _current_reward, _score_min, _rejected\n",
    "        \n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        logging.debug(\"-- Episode : %i\" % i)\n",
    "\n",
    "        state = np.zeros((N_TYPES), dtype=np.uint8) if initial_state is None else initial_state.copy()        \n",
    "        action = get_policy_action(state, action_value_function, epsilon=epsilon)\n",
    "        logging.debug(\"Initial state/action: {}, {}\".format(state, action))\n",
    "\n",
    "        is_terminal, current_reward, score_min, rejected = _is_terminal_state(state, available_gifts, goal_weight)\n",
    "        if is_terminal:\n",
    "            logging.debug(\"Initial state is terminal state. Reward on state: %f\" % current_reward)\n",
    "            state_key = state_to_str(state)\n",
    "            action_value_function[state_key] = NULL_ACTIONS_VALUES\n",
    "            if current_reward == POSITIVE_REWARD:\n",
    "                if best_score < score_min:\n",
    "                    best_score = score_min\n",
    "                    best_state = state\n",
    "            continue\n",
    "\n",
    "        episode_length = 5**N_TYPES                        \n",
    "        while not is_terminal:            \n",
    "            episode_length -= 1 \n",
    "            if episode_length < 0:\n",
    "                logging.warn('Episode length is reached, but state score is still : %f / %f' % (state_score, goal_weight))\n",
    "                break\n",
    "             \n",
    "            new_state = take_action(state, action)\n",
    "            logging.debug(\"New state score, reward, new_state, action : {}, {}, {} <- {}\".format(score_min, current_reward, new_state, action))                \n",
    "\n",
    "            is_terminal, current_reward, score_min, rejected = _is_terminal_state(new_state, available_gifts, goal_weight)\n",
    "                    \n",
    "            if is_terminal:\n",
    "                set_actions_values(new_state, action_value_function, NULL_ACTIONS_VALUES)\n",
    "                if current_reward == POSITIVE_REWARD:\n",
    "                    if best_score < score_min:\n",
    "                        best_score = score_min\n",
    "                        best_state = state\n",
    "\n",
    "    \n",
    "            # Update Q(s,a)\n",
    "            actions_values = get_actions_values(state, action_value_function)\n",
    "            action_value, action_index = find_value(action, actions_values, return_index=True)\n",
    "            # actions_values is a heap with first element being the smallest element\n",
    "            # We store values in actions_values as POSITIVE_REWARD - Q(s,a)            \n",
    "            v = POSITIVE_REWARD - action_value            \n",
    "            new_actions_values = get_actions_values(new_state, action_value_function)            \n",
    "            nv = POSITIVE_REWARD - new_actions_values[0][0] \n",
    "            t = alpha * (current_reward + gamma * nv - v)\n",
    "            actions_values[action_index] = [POSITIVE_REWARD - (v + t), action]\n",
    "            heapq.heapify(actions_values)\n",
    "                            \n",
    "            state = new_state\n",
    "            action = get_policy_action(state, action_value_function, epsilon=epsilon)                        \n",
    "                \n",
    "    return action_value_function, best_score, best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_one_bag(state, action_value_function):\n",
    "    epsilon = 0.0\n",
    "    action = get_policy_action(state, action_value_function, epsilon=epsilon)\n",
    "    actions_values = get_actions_values(state, action_value_function)\n",
    "    value = find_value(action, actions_values)\n",
    "    trajectory = [(state, action, value)]\n",
    "    print trajectory[-1]\n",
    "    counter = 5**N_TYPES\n",
    "    while action is not None:\n",
    "        state = take_action(state, action)\n",
    "        action = get_policy_action(state, action_value_function, epsilon=epsilon)\n",
    "        actions_values = get_actions_values(state, action_value_function)\n",
    "        value = find_value(action, actions_values)\n",
    "        trajectory.append((state, action, value))\n",
    "        print trajectory[-1]\n",
    "\n",
    "        counter -= 1\n",
    "        if counter == 5**N_TYPES - 5:\n",
    "            break\n",
    "            \n",
    "    if counter == 0:\n",
    "        logging.warn(\"Counter is zero\")\n",
    "    return trajectory[-1][0], trajectory\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n"
     ]
    }
   ],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.05\n",
    "alpha = 0.72\n",
    "goal_weight = MAX_WEIGHT * alpha\n",
    "print goal_weight\n",
    "final_action_value_function = defaultdict(list)\n",
    "#final_state = np.zeros((N_TYPES), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_state = np.zeros((N_TYPES), dtype=np.uint8)\n",
    "# final_state = np.array([2, 0, 2, 1, 0, 0, 1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "final_action_value_function, best_score, best_state = q_learning(goal_weight, \n",
    "                                                                 AVAILABLE_GIFTS,\n",
    "                                                                 initial_state=final_state,\n",
    "                                                                 n_episodes=1, \n",
    "                                                                 alpha=0.75, \n",
    "                                                                 gamma=0.85, \n",
    "                                                                 epsilon=0.3, \n",
    "                                                                 action_value_function=final_action_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if best_state is not None:\n",
    "    print best_score, best_state, score((best_state,), return_rejected=True), 2.0*REJECTED_BAGS_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_state = [3, 0, 1, 0, 0, 1, 2, 3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 0, 1, 0, 0, 1, 2, 3, 0],\n",
       " (37.659793454607048,\n",
       "  8.7093288305712786,\n",
       "  0.035000000000000003,\n",
       "  0.18377975949489109))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state, score_stats((best_state,), count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 0, 998.993740136109)\n",
      "(array([1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 4, 998.9008561156111)\n",
      "(array([1, 0, 0, 0, 1, 0, 0, 0, 0], dtype=uint8), 5, 999.0459462926559)\n",
      "(array([1, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 0, 999.2218237690612)\n",
      "(array([2, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 0, 999.2805550588056)\n",
      "(array([3, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 0, 999.6849796989525)\n",
      "[3 0 0 0 1 1 0 0 0] (20.916120757828626, 0.22)\n",
      "[(array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 0, 998.993740136109), (array([1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 4, 998.9008561156111), (array([1, 0, 0, 0, 1, 0, 0, 0, 0], dtype=uint8), 5, 999.0459462926559), (array([1, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 0, 999.2218237690612), (array([2, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 0, 999.2805550588056), (array([3, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8), 0, 999.6849796989525)]\n"
     ]
    }
   ],
   "source": [
    "bag, trajectory = fill_one_bag(final_state, final_action_value_function)\n",
    "print bag, score((bag,), return_rejected=True)\n",
    "print trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'[0, 0, 0, 0, 0, 0, 0, 0, 0]': [[0, 998.7427499601329],\n",
       "              [999.5518973458543, 1],\n",
       "              [999.6170363262567, 2],\n",
       "              [999.3434072343373, 3],\n",
       "              [999.8018738902847, 4],\n",
       "              [999.4477658324171, 5],\n",
       "              [999.3239798818959, 6],\n",
       "              [999.5195041178711, 7],\n",
       "              [999.6776536377635, 8]],\n",
       "             '[1, 0, 0, 0, 0, 0, 0, 0, 0]': [[0, 998.5003591857754],\n",
       "              [999.1658020085647, 1],\n",
       "              [999.2916865191207, 2],\n",
       "              [999.8819951260668, 3],\n",
       "              [999.1812361808566, 4],\n",
       "              [999.3438168070533, 5],\n",
       "              [999.5585201804381, 6],\n",
       "              [999.6323110709333, 7],\n",
       "              [999.7366253942273, 8]],\n",
       "             '[2, 0, 0, 0, 0, 0, 0, 0, 0]': [[0, 998.8925338771791],\n",
       "              [999.3889472353226, 1],\n",
       "              [999.5341535677493, 2],\n",
       "              [999.6258681181616, 3],\n",
       "              [999.642442836396, 4],\n",
       "              [999.3467489232899, 5],\n",
       "              [999.0827016668948, 6],\n",
       "              [999.9397278861974, 7],\n",
       "              [999.5591155924905, 8]],\n",
       "             '[3, 0, 0, 0, 0, 0, 0, 0, 0]': [[999.8268163415623, 0],\n",
       "              [999.2137801649774, 1],\n",
       "              [999.1779587062191, 2],\n",
       "              [999.2793207435212, 3],\n",
       "              [999.9267314813912, 4],\n",
       "              [999.3635535886303, 5],\n",
       "              [999.6511558642619, 6],\n",
       "              [999.4524696237031, 7],\n",
       "              [8, 998.6241971974692]],\n",
       "             '[3, 0, 0, 0, 0, 0, 0, 0, 1]': [[999.3983512756135, 0],\n",
       "              [999.4841547765961, 1],\n",
       "              [999.0480972901095, 2],\n",
       "              [3, 998.6456079928317],\n",
       "              [999.3356298296704, 4],\n",
       "              [999.882433406245, 5],\n",
       "              [999.1778933155614, 6],\n",
       "              [999.6074050214519, 7],\n",
       "              [999.2043971435232, 8]],\n",
       "             '[3, 0, 0, 1, 0, 0, 0, 0, 1]': [[0, 999.0245105258527],\n",
       "              [999.9168262642249, 1],\n",
       "              [999.5736630742681, 2],\n",
       "              [999.7710598917876, 3],\n",
       "              [999.3613890363214, 4],\n",
       "              [999.5376966814138, 5],\n",
       "              [999.0344259255999, 6],\n",
       "              [999.9476374448794, 7],\n",
       "              [999.6176936815278, 8]],\n",
       "             '[4, 0, 0, 1, 0, 0, 0, 0, 1]': [[0, 998.7629619744735],\n",
       "              [999.4525160522238, 1],\n",
       "              [999.988742679909, 2],\n",
       "              [999.9805667328634, 3],\n",
       "              [999.3010696832271, 4],\n",
       "              [999.9506293716111, 5],\n",
       "              [999.6338802178844, 6],\n",
       "              [999.138361573143, 7],\n",
       "              [999.4742516891897, 8]],\n",
       "             '[5, 0, 0, 1, 0, 0, 0, 0, 1]': [[0, 998.9291201165518],\n",
       "              [999.62140544267, 1],\n",
       "              [999.8704970721041, 2],\n",
       "              [999.870432180838, 3],\n",
       "              [999.1003014779799, 4],\n",
       "              [999.5003631759471, 5],\n",
       "              [999.7536696331902, 6],\n",
       "              [999.8562420980227, 7],\n",
       "              [999.4329977988732, 8]],\n",
       "             '[6, 0, 0, 1, 0, 0, 0, 0, 1]': [[999.7910612622272, 0],\n",
       "              [1, 4749.978387086678],\n",
       "              [999.2750909176746, 2],\n",
       "              [999.6876495411642, 3],\n",
       "              [999.517469876038, 4],\n",
       "              [999.5915944175749, 5],\n",
       "              [999.008112146111, 6],\n",
       "              [999.5493571788596, 7],\n",
       "              [999.5712560772228, 8]],\n",
       "             '[6, 1, 0, 1, 0, 0, 0, 0, 1]': [(1000, None)]})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for count in [100, 200, 300]:\n",
    "#     sc = []\n",
    "#     sc2 = []\n",
    "#     for i in range(200):\n",
    "#         s, r = score((best_state,), return_rejected=True, count=count)\n",
    "#         sc.append(s)\n",
    "#         rr.append(r)\n",
    "\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(131)    \n",
    "#     plt.plot(sc)\n",
    "#     plt.subplot(132)\n",
    "#     plt.plot(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value function estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0\n"
     ]
    }
   ],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.05\n",
    "alpha = 0.76\n",
    "goal_weight = MAX_WEIGHT * alpha\n",
    "print goal_weight\n",
    "\n",
    "filled_bags = np.zeros((N_BAGS, N_TYPES), dtype=np.uint8)\n",
    "final_action_value_function = defaultdict(list)\n",
    "available_gifts = deepcopy(AVAILABLE_GIFTS)\n",
    "bag_index = 0\n",
    "initial_state = filled_bags[0]\n",
    "# found_goal_states = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Filled bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 0, '/', 1000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-931cd68dcf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                                  \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                  \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                                  action_value_function=final_action_value_function)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"- Got a result : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f138a3e93e94>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(goal_weight, available_gifts, initial_state, n_episodes, alpha, gamma, epsilon, action_value_function)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcurrent_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mstate_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_score_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mscore_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstate_score_std\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mscore_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_score_std\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/Santas-Uncertain-Bags/common/utils.py\u001b[0m in \u001b[0;36mscore_stats\u001b[0;34m(state, count, max_weight)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mrejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mtotal_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight_\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_weight_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/Santas-Uncertain-Bags/common/utils.py\u001b[0m in \u001b[0;36mbag_weight\u001b[0;34m(bag, n1)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/Santas-Uncertain-Bags/common/utils.py\u001b[0m in \u001b[0;36mweight3\u001b[0;34m(index, count)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2942\u001b[0;31m                             out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.WARN)\n",
    "n_episodes = 250\n",
    "\n",
    "last_score_computation = -1\n",
    "while bag_index < N_BAGS:\n",
    "    \n",
    "    print(\"Filled bags : \", bag_index, \"/\", N_BAGS)\n",
    "    \n",
    "    final_action_value_function, best_score, best_state = q_learning(goal_weight, \n",
    "                                                                 available_gifts,\n",
    "                                                                 initial_state=initial_state,\n",
    "                                                                 n_episodes=n_episodes, \n",
    "                                                                 alpha=0.75, \n",
    "                                                                 gamma=0.85, \n",
    "                                                                 epsilon=0.25, \n",
    "                                                                 action_value_function=final_action_value_function)\n",
    "    if best_score > 0:\n",
    "        print(\"- Got a result : \", best_score, best_state)\n",
    "        update_available_gifts(available_gifts, best_state, GIFT_TYPES)\n",
    "        \n",
    "#         if len(found_goal_states) == 0 or found_goal_states[-1] != result.state:\n",
    "#             found_goal_states.append(result.state)\n",
    "        initial_state = best_state\n",
    "    \n",
    "        filled_bags[bag_index, :] = best_state[:]\n",
    "        bag_index += 1\n",
    "    else:\n",
    "        print(\"No best state found\")\n",
    "        \n",
    "        \n",
    "    if bag_index > 0 and (bag_index % 20) == 0 and last_score_computation < bag_index:\n",
    "            s, r = score(filled_bags, return_rejected=True)\n",
    "            print(\">>> Current score: \", s, s * N_BAGS *1.0 / bag_index, \"rejected=\", r)\n",
    "            last_score_computation = bag_index\n",
    "\n",
    "    if bag_index > 0 and (bag_index % 30) == 0 and last_score_computation < bag_index:\n",
    "        print(\">>> Currently available gifts : \", [(k, available_gifts[k]) for k in GIFT_TYPES])\n",
    "        last_score_computation = bag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112.7013124911667, 0.17999999999999999)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(filled_bags, return_rejected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
