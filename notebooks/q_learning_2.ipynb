{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Q-learning tryout on Santa's uncertain bags\n",
    "\n",
    "We reduce the number of possible states corresponding to 1000 bags to states corresponding to 1 bag. Problem of filling 1000 bags will be translated to the optimal usage of policy and action-value function on varying environment. The last is given by the array of available gifts which decreases when bags are filled.\n",
    "\n",
    "\n",
    "## States \n",
    "\n",
    "A state is characterized by a vector of size `(N_TYPES)`. For example, `s=[1,0,1,0,0,0,0,0,0]`. The initial state is when the null vector or a customly defined vector. Terminal states are defined by state's score. \n",
    "\n",
    "How many state there are? There are at most `10^N_TYPES` states.\n",
    "\n",
    "\n",
    "## Actions\n",
    "\n",
    "Action is to add a toy to the bag following the list of available toys. For example, action is a integer value corresponding to the toy index.\n",
    "\n",
    "\n",
    "## Rewards\n",
    "\n",
    "Action reward can be defined by the score of the bag where a toy has been added.\n",
    "\n",
    "\n",
    "## Q-learning: Off-Policy Temporal Difference Control\n",
    "\n",
    "In this algorithm we estimate action-value function $Q(s,a)$ as :\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t,A_t) \\right], \\, Q(\\cal{S}^{+},a)=0\n",
    "$$\n",
    "\n",
    "**Algorithm**\n",
    "<br>\n",
    "<div style=\"background-color: #aaaaaa; padding: 10px; width: 75%; border: solid black; border-radius: 5px;\">\n",
    "\n",
    "    Initialize $Q(s, a)$, for all $s \\in \\cal{S}$, $a \\in \\cal{A}(s)$, arbitrarily, and $Q(\\text{terminal-state}, \\cdot) = 0$<br>\n",
    "    Repeat (for each episode):<br>\n",
    "    &emsp;Initialize $S$<br>\n",
    "    &emsp;Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)<br>\n",
    "    &emsp;Repeat (for each step of episode):<br>\n",
    "    &emsp;&emsp;Take action $A$, observe $R$, $S'$<br>\n",
    "    &emsp;&emsp;$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\max_{a}Q(S', a) - Q(S,A) \\right]$<br>\n",
    "    &emsp;&emsp;$S \\leftarrow S'; \\, A \\leftarrow A';$<br>\n",
    "    &emsp;until $S$ is terminal\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(2017)\n",
    "\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from utils import weight3 as weight_fn, weight_by_index\n",
    "from utils import bag_weight, score, mean_n_sigma, score_stats\n",
    "from utils import MAX_WEIGHT, AVAILABLE_GIFTS, GIFT_TYPES, N_TYPES, N_BAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.015\n",
    "NEGATIVE_REWARD = -1000\n",
    "POSITIVE_REWARD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step_reward(rejected):    \n",
    "    return 1.0 if rejected < REJECTED_BAGS_THRESHOLD else -rejected*10\n",
    "\n",
    "def take_action(state, action):\n",
    "    new_state = state.copy()\n",
    "    new_state[action] += 1\n",
    "    return new_state\n",
    "\n",
    "def is_available(state, available_gifts, gift_types=GIFT_TYPES):\n",
    "    for v, gift_type in zip(state, gift_types):\n",
    "        if available_gifts[gift_type] - v < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def update_available_gifts(available_gifts, state, gift_types=GIFT_TYPES):\n",
    "    for v, gift_type in zip(state, gift_types):\n",
    "        assert available_gifts[gift_type] - v >= 0, \"Found state is not available : {}, {}\".format(state, available_gifts)\n",
    "        available_gifts[gift_type] = available_gifts[gift_type] - v\n",
    "        \n",
    "def state_to_str(state):\n",
    "    return state.tolist().__str__()\n",
    "\n",
    "def find_value(action, actions_values, return_index=False):\n",
    "    for i, (v, a) in enumerate(actions_values):\n",
    "        if action == a:\n",
    "            if return_index:\n",
    "                return v, i\n",
    "            return v\n",
    "    raise Exception(\"No action={} in actions_values={}\".format(action, actions_values))\n",
    "    \n",
    "def has_action(actions_values, action, return_index=False):\n",
    "    for i, (v, a) in enumerate(actions_values):\n",
    "        if action == a:\n",
    "            if return_index:\n",
    "                return True, i\n",
    "            return True\n",
    "    if return_index:\n",
    "        return False, None\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_policy_action(state, action_value_function, epsilon=0.1):\n",
    "    state_key = state_to_str(state)\n",
    "    u = np.random.rand()\n",
    "    b = state_key in action_value_function\n",
    "    if b and u > epsilon:\n",
    "        # Get max value action\n",
    "        actions_values = action_value_function[state_key]\n",
    "        max_action_value = actions_values[0]\n",
    "        return max_action_value[1]\n",
    "    else:\n",
    "        # Take a random action\n",
    "        action = np.random.randint(N_TYPES)\n",
    "        b1 = False\n",
    "        if b:\n",
    "            b1 = has_action(action_value_function[state_key], action)\n",
    "        if not b or not b1:\n",
    "            # Arbitrary initialization\n",
    "            # We store values as POSITIVE_REWARD - value to use heapq property that heap[0] is the smallest element\n",
    "            # In our case this element corresponds to the largest value\n",
    "            value = POSITIVE_REWARD - np.random.rand()\n",
    "            heapq.heappush(action_value_function[state_key], [value, action])                     \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_learning(goal_weight, \n",
    "               available_gifts,\n",
    "               initial_state=None,\n",
    "               n_episodes=10, alpha=0.75, gamma=0.95, epsilon=0.1, action_value_function=None):\n",
    "    \n",
    "    logging.info(\"--- Q-learning : goal={}, n_episodes={}\".format(goal_weight, n_episodes))\n",
    "    if action_value_function is None:\n",
    "        logging.info(\"-- Reset action_value_function\")\n",
    "        action_value_function = defaultdict(list)\n",
    "    \n",
    "    best_state = initial_state\n",
    "    best_score = 0\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        logging.debug(\"-- Episode : %i\" % i)\n",
    "        \n",
    "        episode_length = 5**N_TYPES\n",
    "        state = np.zeros((N_TYPES), dtype=np.uint8) if initial_state is None else initial_state        \n",
    "        action = get_policy_action(state, action_value_function, epsilon=epsilon)\n",
    "        state_score, state_score_std, rejected, rejected_std = score_stats((best_state,), count=200)\n",
    "        #state_score, rejected = score((state,), return_rejected=True)\n",
    "        score_min = state_score - state_score_std*0.1\n",
    "        score_max = state_score + state_score_std*0.5\n",
    "        is_terminal = score_min > goal_weight and is_available(state, available_gifts)\n",
    "        is_terminal |= rejected + rejected_std*0.25 > 2.0*REJECTED_BAGS_THRESHOLD\n",
    "        \n",
    "        logging.debug(\"Initial state score/action: {}, {}\".format(state_score, action))\n",
    "        \n",
    "        while not is_terminal:\n",
    "            \n",
    "            episode_length -= 1 \n",
    "            if episode_length < 0:\n",
    "                logging.warn('Episode length is reached, but state score is still : %f / %f' % (state_score, goal_weight))\n",
    "                break\n",
    "            \n",
    "            current_reward = 0 \n",
    "            new_state = take_action(state, action)\n",
    "            state_score, state_score_std, rejected, rejected_std = score_stats((new_state,), count=200)\n",
    "            score_min = state_score - state_score_std*0.1\n",
    "            score_max = state_score + state_score_std*0.5            \n",
    "            rejected += rejected_std*0.25\n",
    "            \n",
    "            if not is_available(new_state, available_gifts) or rejected > 2.0*REJECTED_BAGS_THRESHOLD:                \n",
    "                current_reward = NEGATIVE_REWARD\n",
    "                is_terminal = True\n",
    "                logging.debug(\"--->1 Episode finished with NEGATIVE reward, {}, {}, {}\".format(score_min, score_max, rejected))\n",
    "            elif score_max >= MAX_WEIGHT:\n",
    "                current_reward = NEGATIVE_REWARD\n",
    "                is_terminal = True\n",
    "                logging.debug(\"--->2 Episode finished with NEGATIVE reward, {}, {}, {}\".format(score_min, score_max, rejected))\n",
    "            elif MAX_WEIGHT > score_min >= goal_weight:\n",
    "                current_reward = POSITIVE_REWARD\n",
    "                is_terminal = True\n",
    "                logging.debug(\"---> Episode finished with POSITIVE reward\")\n",
    "                if best_score < score_min:\n",
    "                    best_score = score_min\n",
    "                    best_state = new_state\n",
    "            elif score_min < goal_weight:\n",
    "                current_reward = step_reward(rejected)\n",
    "            else:\n",
    "                raise Exception(\"Unclassified state: {}, score_min={}, score_max={}, rejected={}\".format(new_state, score_min, score_max, rejected))\n",
    "\n",
    "            # logging.debug(\"New state score, reward, new_state, action : {}, {}, {}, {}\".format(state_score, current_reward, new_state, action))                \n",
    "                \n",
    "            # Update Q(s,a)\n",
    "            state_key = state_to_str(state)\n",
    "            new_state_key = state_to_str(new_state)\n",
    "            \n",
    "            actions_values = action_value_function[state_key]\n",
    "            action_value, action_index = find_value(action, actions_values, return_index=True)\n",
    "            v = POSITIVE_REWARD - action_value\n",
    "            # actions_values is a heap with first element being the smallest element\n",
    "            # We store values in actions_values as POSITIVE_REWARD - Q(s,a)\n",
    "            nv = POSITIVE_REWARD - actions_values[0][0]\n",
    "            t = alpha * (current_reward + gamma * nv - v)\n",
    "            \n",
    "            action_value_function[state_key][action_index] = [POSITIVE_REWARD - (v + t), action]\n",
    "            \n",
    "            state = new_state\n",
    "            action = get_policy_action(state, action_value_function, epsilon=epsilon)                        \n",
    "                \n",
    "    return action_value_function, best_score, best_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.5\n"
     ]
    }
   ],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.05\n",
    "alpha = 0.75\n",
    "goal_weight = MAX_WEIGHT * alpha\n",
    "print goal_weight\n",
    "final_action_value_function = defaultdict(list)\n",
    "$final_state = np.zeros((N_TYPES), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_state = np.zeros((N_TYPES), dtype=np.uint8)\n",
    "final_state = np.array([2, 0, 2, 1, 0, 0, 1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:--- Q-learning : goal=38.0, n_episodes=100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-817-24dad780fc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                  \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                  \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                                  action_value_function=final_action_value_function)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-779-f138a3e93e94>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(goal_weight, available_gifts, initial_state, n_episodes, alpha, gamma, epsilon, action_value_function)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mN_TYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_policy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_value_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mstate_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_score_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#state_score, rejected = score((state,), return_rejected=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-1280378aa41a>\u001b[0m in \u001b[0;36mget_policy_action\u001b[0;34m(state, action_value_function, epsilon)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_policy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_value_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstate_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_value_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-b954ed9089c0>\u001b[0m in \u001b[0;36mstate_to_str\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstate_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "final_action_value_function, best_score, best_state = q_learning(goal_weight, \n",
    "                                                                 AVAILABLE_GIFTS,\n",
    "                                                                 initial_state=final_state,\n",
    "                                                                 n_episodes=100, \n",
    "                                                                 alpha=0.75, \n",
    "                                                                 gamma=0.85, \n",
    "                                                                 epsilon=0.3, \n",
    "                                                                 action_value_function=final_action_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.788062681555829,\n",
       " array([3, 0, 1, 0, 0, 1, 2, 3, 0], dtype=uint8),\n",
       " (38.243957462353478, 0.02),\n",
       " 0.1)"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_state, score((best_state,), return_rejected=True), 2.0*REJECTED_BAGS_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_state = [3, 0, 1, 0, 0, 1, 2, 3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 0, 1, 0, 0, 1, 2, 3, 0], dtype=uint8),\n",
       " (36.561601141292897,\n",
       "  10.630658920860142,\n",
       "  0.065000000000000002,\n",
       "  0.246525860712421))"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state, score_stats((best_state,), count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for count in [100, 200, 300]:\n",
    "#     sc = []\n",
    "#     sc2 = []\n",
    "#     for i in range(200):\n",
    "#         s, r = score((best_state,), return_rejected=True, count=count)\n",
    "#         sc.append(s)\n",
    "#         rr.append(r)\n",
    "\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(131)    \n",
    "#     plt.plot(sc)\n",
    "#     plt.subplot(132)\n",
    "#     plt.plot(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value function estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0\n"
     ]
    }
   ],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.05\n",
    "alpha = 0.76\n",
    "goal_weight = MAX_WEIGHT * alpha\n",
    "print goal_weight\n",
    "\n",
    "filled_bags = np.zeros((N_BAGS, N_TYPES), dtype=np.uint8)\n",
    "final_action_value_function = defaultdict(list)\n",
    "available_gifts = deepcopy(AVAILABLE_GIFTS)\n",
    "bag_index = 0\n",
    "initial_state = filled_bags[0]\n",
    "# found_goal_states = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Filled bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 0, '/', 1000)\n",
      "('- Got a result : ', 38.528628281654768, array([2, 0, 2, 1, 0, 0, 1, 2, 0], dtype=uint8))\n",
      "('Filled bags : ', 1, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 1, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 1, '/', 1000)\n",
      "No best state found\n",
      "('Filled bags : ', 1, '/', 1000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-815-931cd68dcf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                                  \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                  \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                                  action_value_function=final_action_value_function)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"- Got a result : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-779-f138a3e93e94>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(goal_weight, available_gifts, initial_state, n_episodes, alpha, gamma, epsilon, action_value_function)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_policy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_value_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mstate_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_score_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#state_score, rejected = score((state,), return_rejected=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mscore_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstate_score_std\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/Santas-Uncertain-Bags/common/utils.py\u001b[0m in \u001b[0;36mscore_stats\u001b[0;34m(state, count, max_weight)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mrejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mtotal_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight_\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_weight_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/Santas-Uncertain-Bags/common/utils.py\u001b[0m in \u001b[0;36mbag_weight\u001b[0;34m(bag, n1)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/Santas-Uncertain-Bags/common/utils.py\u001b[0m in \u001b[0;36mweight3\u001b[0;34m(index, count)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/venv/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2942\u001b[0;31m                             out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/venv/local/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.WARN)\n",
    "n_episodes = 250\n",
    "\n",
    "last_score_computation = -1\n",
    "while bag_index < N_BAGS:\n",
    "    \n",
    "    print(\"Filled bags : \", bag_index, \"/\", N_BAGS)\n",
    "    \n",
    "    final_action_value_function, best_score, best_state = q_learning(goal_weight, \n",
    "                                                                 available_gifts,\n",
    "                                                                 initial_state=initial_state,\n",
    "                                                                 n_episodes=n_episodes, \n",
    "                                                                 alpha=0.75, \n",
    "                                                                 gamma=0.85, \n",
    "                                                                 epsilon=0.25, \n",
    "                                                                 action_value_function=final_action_value_function)\n",
    "    if best_score > 0:\n",
    "        print(\"- Got a result : \", best_score, best_state)\n",
    "        update_available_gifts(available_gifts, best_state, GIFT_TYPES)\n",
    "        \n",
    "#         if len(found_goal_states) == 0 or found_goal_states[-1] != result.state:\n",
    "#             found_goal_states.append(result.state)\n",
    "        initial_state = best_state\n",
    "    \n",
    "        filled_bags[bag_index, :] = best_state[:]\n",
    "        bag_index += 1\n",
    "    else:\n",
    "        print(\"No best state found\")\n",
    "        \n",
    "        \n",
    "    if bag_index > 0 and (bag_index % 20) == 0 and last_score_computation < bag_index:\n",
    "            s, r = score(filled_bags, return_rejected=True)\n",
    "            print(\">>> Current score: \", s, s * N_BAGS *1.0 / bag_index, \"rejected=\", r)\n",
    "            last_score_computation = bag_index\n",
    "\n",
    "    if bag_index > 0 and (bag_index % 30) == 0 and last_score_computation < bag_index:\n",
    "        print(\">>> Currently available gifts : \", [(k, available_gifts[k]) for k in GIFT_TYPES])\n",
    "        last_score_computation = bag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112.7013124911667, 0.17999999999999999)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(filled_bags, return_rejected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
