{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Q-learning tryout on Santa's uncertain bags\n",
    "\n",
    "We reduce the number of possible states corresponding to 1000 bags to states corresponding to 1 bag. Problem of filling 1000 bags will be translated to the optimal usage of policy and action-value function on varying environment. The last is given by the array of available gifts which decreases when bags are filled.\n",
    "\n",
    "\n",
    "## States \n",
    "\n",
    "A state is characterized by a vector of size `(N_TYPES)`. For example, `s=[1,0,1,0,0,0,0,0,0]`. The initial state is when the null vector or a customly defined vector. Terminal states are defined by state's score. \n",
    "\n",
    "How many state there are? There are at most `10^N_TYPES` states.\n",
    "\n",
    "\n",
    "## Actions\n",
    "\n",
    "Action is to add a toy to the bag following the list of available toys. For example, action is a integer value corresponding to the toy index.\n",
    "\n",
    "\n",
    "## Rewards\n",
    "\n",
    "Action reward can be defined by the score of the bag where a toy has been added.\n",
    "\n",
    "\n",
    "## Q-learning: Off-Policy Temporal Difference Control\n",
    "\n",
    "In this algorithm we estimate action-value function $Q(s,a)$ as :\n",
    "$$\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t,A_t) \\right], \\, Q(\\cal{S}^{+},a)=0\n",
    "$$\n",
    "\n",
    "**Algorithm**\n",
    "<br>\n",
    "<div style=\"background-color: #aaaaaa; padding: 10px; width: 75%; border: solid black; border-radius: 5px;\">\n",
    "\n",
    "    Initialize $Q(s, a)$, for all $s \\in \\cal{S}$, $a \\in \\cal{A}(s)$, arbitrarily, and $Q(\\text{terminal-state}, \\cdot) = 0$<br>\n",
    "    Repeat (for each episode):<br>\n",
    "    &emsp;Initialize $S$<br>\n",
    "    &emsp;Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)<br>\n",
    "    &emsp;Repeat (for each step of episode):<br>\n",
    "    &emsp;&emsp;Take action $A$, observe $R$, $S'$<br>\n",
    "    &emsp;&emsp;$Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R + \\gamma \\max_{a}Q(S', a) - Q(S,A) \\right]$<br>\n",
    "    &emsp;&emsp;$S \\leftarrow S'; \\, A \\leftarrow A';$<br>\n",
    "    &emsp;until $S$ is terminal\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(2017)\n",
    "\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from utils import weight3 as weight_fn, weight_by_index\n",
    "from utils import bag_weight, score, mean_n_sigma, score_stats\n",
    "from utils import MAX_WEIGHT, AVAILABLE_GIFTS, GIFT_TYPES, N_TYPES, N_BAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute maximal number of gifts in a terminal state for each product :\n",
    "\n",
    "$$ (\\mathbb{E}[Y] - \\text{Var}[Y]^{1/2}) = \\text{max_bag_weight}, \\, Y=N_i \\cdot m_i$$\n",
    "or \n",
    "$$ N_i = \\lceil \\frac{\\text{max_bag_weight}}{\\mathbb{E}[X_i] - \\text{Var}[X_i]^{1/2}} \\rceil$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 34,  14,   8, 218,   3,  26, 184,  30,  28], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIMIT_STATE = np.zeros((N_TYPES), dtype=np.uint8)\n",
    "indices = list(range(N_TYPES))\n",
    "indices.remove(4)\n",
    "for i in indices:\n",
    "    weights = [weight_by_index(i) for j in range(50000)]    \n",
    "    v = np.percentile(weights, 5)\n",
    "    LIMIT_STATE[i] = int(np.ceil(MAX_WEIGHT/v))\n",
    "    \n",
    "v = 24.0741557491\n",
    "LIMIT_STATE[4] = int(np.ceil(MAX_WEIGHT/v))\n",
    "    \n",
    "LIMIT_STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max number of states :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10007950417920"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(LIMIT_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ball': 1.99876912083,\n",
       " 'bike': 20.0021364556,\n",
       " 'blocks': 11.6630321858,\n",
       " 'book': 2.00086596571,\n",
       " 'coal': 23.7866257713,\n",
       " 'doll': 4.9993625282,\n",
       " 'gloves': 1.40310067709,\n",
       " 'horse': 4.99527064522,\n",
       " 'train': 10.0234458084}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_weights = {}\n",
    "fixed_weights['ball'] = 1.99876912083\n",
    "fixed_weights['bike'] = 20.0021364556\n",
    "fixed_weights['blocks'] = 11.6630321858\n",
    "fixed_weights['book'] = 2.00086596571\n",
    "fixed_weights['coal'] = 23.7866257713\n",
    "fixed_weights['doll'] = 4.9993625282\n",
    "fixed_weights['gloves'] = 1.40310067709\n",
    "fixed_weights['horse'] = 4.99527064522\n",
    "fixed_weights['train'] = 10.0234458084\n",
    "fixed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TRIALS = 10000\n",
    "GIFT_WEIGHTS = np.zeros((N_TRIALS, N_TYPES))\n",
    "for index in range(N_TYPES):\n",
    "    GIFT_WEIGHTS[:, index] = [weight_by_index(index) for i in range(10000)]\n",
    "    \n",
    "def compute_score(state):\n",
    "    s = np.sum(GIFT_WEIGHTS * state, axis=1)\n",
    "    mask = s < MAX_WEIGHT\n",
    "    rejected = (N_TRIALS - np.sum(mask))*1.0 / N_TRIALS\n",
    "    score = np.sum(s[mask]) * 1.0 / N_TRIALS\n",
    "    return score, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.015\n",
    "NEGATIVE_REWARD = -5000\n",
    "POSITIVE_REWARD = 1000\n",
    "STEP_POSITIVE_REWARD = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step_reward(rejected, state):    \n",
    "    r = STEP_POSITIVE_REWARD if rejected < REJECTED_BAGS_THRESHOLD else -rejected*10\n",
    "    #r += np.sum(state)**2\n",
    "    return r \n",
    "\n",
    "def take_action(state, action):\n",
    "    if action is None:\n",
    "        return state\n",
    "    new_state = state.copy()\n",
    "    new_state[action] += 1\n",
    "    return new_state\n",
    "\n",
    "def is_available(state, available_gifts, gift_types=GIFT_TYPES):\n",
    "    for v, gift_type in zip(state, gift_types):\n",
    "        if available_gifts[gift_type] - v < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def update_available_gifts(available_gifts, state, gift_types=GIFT_TYPES):\n",
    "    for v, gift_type in zip(state, gift_types):\n",
    "        assert available_gifts[gift_type] - v >= 0, \"Found state is not available : {}, {}\".format(state, available_gifts)\n",
    "        available_gifts[gift_type] = available_gifts[gift_type] - v\n",
    "        \n",
    "def state_to_str(state):\n",
    "    return state.tolist().__str__()\n",
    "\n",
    "def find_value(action, actions_values, return_index=False):\n",
    "    for i, (v, a) in enumerate(actions_values):\n",
    "        if action == a:\n",
    "            if return_index:\n",
    "                return v, i\n",
    "            return v\n",
    "    raise Exception(\"No action={} in actions_values={}\".format(action, actions_values))\n",
    "    \n",
    "def has_action(actions_values, action, return_index=False):\n",
    "    for i, (v, a) in enumerate(actions_values):\n",
    "        if action == a:\n",
    "            if return_index:\n",
    "                return True, i\n",
    "            return True\n",
    "    if return_index:\n",
    "        return False, None\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NULL_ACTIONS_VALUES = [(0, None)]\n",
    "\n",
    "def bag_fix_weight(state):\n",
    "    out = 0\n",
    "    for i, c in enumerate(state):\n",
    "        out += fixed_weights[GIFT_TYPES[i]] * c\n",
    "    return out\n",
    "\n",
    "def is_too_heavy(state):\n",
    "    b1 = (LIMIT_STATE - state < 0).any()\n",
    "    b2 = bag_fix_weight(state) > MAX_WEIGHT\n",
    "    return b1 or b2\n",
    "\n",
    "\n",
    "def explore_good_actions(actions_values, epsilon):\n",
    "    a = np.array(actions_values)\n",
    "    mask = a < 0; mask = np.sum(mask, axis=1).astype(bool)\n",
    "#     print '1 explore_good_actions: ', mask, a[mask]\n",
    "#     print '2 explore_good_actions: ', a[mask][:, 1]\n",
    "    actions = a[mask][:, 1]\n",
    "    if len(actions) == 1 and actions[0] is None:\n",
    "        # If actions values of a terminal state\n",
    "        return None\n",
    "    elif len(actions) == 1:\n",
    "        return int(actions[0])\n",
    "    elif len(actions) == 0:\n",
    "        # No good actions -> return the best among not good actions\n",
    "        return actions_values[0][1]\n",
    "    \n",
    "    actions = actions.astype(np.uint8).tolist()\n",
    "    ll = len(actions)\n",
    "    actions.remove(actions_values[0][1])\n",
    "    pr = 1.0 - epsilon + epsilon / ll\n",
    "    if np.random.rand() <= pr:\n",
    "        return actions_values[0][1]\n",
    "    else:\n",
    "        return actions[np.random.randint(ll-1)]\n",
    "\n",
    "def get_actions_values(state, action_value_function):\n",
    "    state_key = state_to_str(state)\n",
    "    \n",
    "    actions_values = action_value_function[state_key]\n",
    "    if len(actions_values) == 0:\n",
    "#         logging.debug(\"get_actions_values : Initialize random values\")\n",
    "#         for i in range(N_TYPES):\n",
    "#             va = [-np.random.rand(), i]            \n",
    "#             heapq.heappush(action_value_function[state_key], va)        \n",
    "        logging.debug(\"get_actions_values : Initialize values\")\n",
    "        index = np.random.randint(N_TYPES)\n",
    "        indices = list(range(N_TYPES))\n",
    "        indices.remove(index)\n",
    "        va = [-1, index]            \n",
    "        heapq.heappush(action_value_function[state_key], va)        \n",
    "        for i in indices:\n",
    "            va = [0, i]            \n",
    "            heapq.heappush(action_value_function[state_key], va)        \n",
    "    return action_value_function[state_key]    \n",
    "\n",
    "def set_actions_values(state, action_value_function, actions_values):\n",
    "    state_key = state_to_str(state)\n",
    "    action_value_function[state_key] = deepcopy(actions_values)\n",
    "\n",
    "def get_policy_action(state, action_value_function, available_gifts, epsilon=0.1, ksi=0.75):    \n",
    "    u = np.random.rand()\n",
    "    # Get max value action\n",
    "    actions_values = get_actions_values(state, action_value_function)\n",
    "    max_action_value = actions_values[0]\n",
    "    pr = 1.0 - epsilon + epsilon / N_TYPES\n",
    "    if u <= pr:\n",
    "        # Explore other good actions\n",
    "        action = explore_good_actions(actions_values, ksi)\n",
    "        #action = max_action_value[1]        \n",
    "        new_state = take_action(state, action)\n",
    "        count = 1\n",
    "        while not is_available(new_state, available_gifts) and count < len(actions_values):\n",
    "            action = actions_values[count][1]        \n",
    "            new_state = take_action(state, action)\n",
    "            count += 1    \n",
    "        if not is_available(new_state, available_gifts):\n",
    "            logging.debug(\"get_policy_action: 1 return action=None, actions_values={}\".format(actions_values))\n",
    "            return None\n",
    "        return action\n",
    "        \n",
    "    else:\n",
    "        # Exploring\n",
    "        if max_action_value[1] is None:\n",
    "            logging.debug(\"get_policy_action: 2 return action=None, actions_values={}\".format(actions_values))\n",
    "            return None\n",
    "        \n",
    "        actions = list(range(N_TYPES))\n",
    "        actions.remove(max_action_value[1])\n",
    "        \n",
    "        action = actions[np.random.randint(N_TYPES-1)]\n",
    "        new_state = take_action(state, action)\n",
    "        count = 0\n",
    "        while not is_available(new_state, available_gifts) and count < N_TYPES-1:\n",
    "            action = actions[count]\n",
    "            new_state = take_action(state, action)\n",
    "            count += 1  \n",
    "        if not is_available(new_state, available_gifts):\n",
    "            # Check if max action value is OK\n",
    "            action = max_action_value[1]        \n",
    "            new_state = take_action(state, action)\n",
    "            if not is_available(new_state, available_gifts):\n",
    "                logging.debug(\"get_policy_action: 3 return action=None, actions_values={}\".format(actions_values))\n",
    "                return None\n",
    "            return action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_learning(goal_weight, \n",
    "               available_gifts,\n",
    "               initial_state=None,\n",
    "               n_episodes=10, alpha=0.75, gamma=0.95, epsilon=0.1, action_value_function=None):\n",
    "    \n",
    "    logging.info(\"--- Q-learning : goal={}, n_episodes={}\".format(goal_weight, n_episodes))\n",
    "    if action_value_function is None:\n",
    "        logging.info(\"-- Reset action_value_function\")\n",
    "        action_value_function = defaultdict(list)\n",
    "    \n",
    "    best_state = None\n",
    "    best_score = 0\n",
    "    best_rejected = 0.0\n",
    "    \n",
    "    def _is_terminal_state(state, goal_weight):\n",
    "        _is_terminal = False\n",
    "        _current_reward = 0\n",
    "        \n",
    "        _state_score, _rejected = compute_score(state)\n",
    "        if _rejected > 10.0*REJECTED_BAGS_THRESHOLD:                \n",
    "            _current_reward = NEGATIVE_REWARD - (MAX_WEIGHT - _state_score)\n",
    "            _is_terminal = True\n",
    "            logging.debug(\"--->1 Episode finished with NEGATIVE reward, {}, {}\".format(_state_score, _rejected))                \n",
    "#         elif _state_score >= MAX_WEIGHT:\n",
    "#             _current_reward = NEGATIVE_REWARD\n",
    "#             _is_terminal = True\n",
    "#             logging.debug(\"--->2 Episode finished with NEGATIVE reward, {}, {}, {}\".format(_state_score, _rejected))\n",
    "        elif MAX_WEIGHT > _state_score >= goal_weight:\n",
    "            _current_reward = POSITIVE_REWARD + _state_score + np.sum(state)**2\n",
    "            _is_terminal = False\n",
    "#             logging.debug(\"---> Episode finished with POSITIVE reward\")\n",
    "        elif _state_score < goal_weight:\n",
    "            _current_reward = step_reward(_rejected, state)\n",
    "        else:\n",
    "            raise Exception(\"Unclassified state: {}, score_min={}, score_max={}, rejected={}\".format(new_state, score_min, score_max, rejected))\n",
    "\n",
    "        return _is_terminal, _current_reward, _state_score, _rejected\n",
    "        \n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        logging.debug(\"-- Episode : %i\" % i)\n",
    "\n",
    "        state = np.zeros((N_TYPES), dtype=np.uint8) if initial_state is None else initial_state.copy()        \n",
    "        action = get_policy_action(state, action_value_function, available_gifts, epsilon=epsilon)\n",
    "        logging.debug(\"Initial state/action: {}, {}\".format(state, action))\n",
    "\n",
    "        is_terminal, current_reward, score_min, rejected = _is_terminal_state(state, goal_weight)\n",
    "        if current_reward >= POSITIVE_REWARD:\n",
    "            if best_score < score_min:\n",
    "                best_score = score_min\n",
    "                best_state = state\n",
    "                best_rejected = rejected\n",
    "        if is_terminal:\n",
    "            logging.debug(\"Initial state is terminal state. Reward on state: %f\" % current_reward)\n",
    "            set_actions_values(state, action_value_function, NULL_ACTIONS_VALUES)\n",
    "            continue\n",
    "\n",
    "        episode_length = 5**N_TYPES                        \n",
    "        while not is_terminal:            \n",
    "            episode_length -= 1 \n",
    "            if episode_length < 0:\n",
    "                logging.warn('Episode length is reached, but state score is still : %f / %f' % (state_score, goal_weight))\n",
    "                break\n",
    "             \n",
    "            new_state = take_action(state, action)\n",
    "            is_terminal, current_reward, score_min, rejected = _is_terminal_state(new_state, goal_weight)\n",
    "            logging.debug(\"New state score, reward, new_state, action : {}, {}, {} <- {}\".format(score_min, current_reward, new_state, action))                \n",
    "\n",
    "#             print(\"\\n --- New state score, reward, new_state, action : {}, {}, {} <- {}\".format(score_min, current_reward, new_state, action))                \n",
    "#             print(\"get_actions_values(state, action_value_function): \", get_actions_values(state, action_value_function))\n",
    "#             print(\"get_actions_values(new_state, action_value_function): \", get_actions_values(new_state, action_value_function))\n",
    "            \n",
    "            if is_terminal:\n",
    "#                 print \"--- Terminal state is found ---\"\n",
    "                set_actions_values(new_state, action_value_function, NULL_ACTIONS_VALUES)\n",
    "            if current_reward >= POSITIVE_REWARD:\n",
    "                if best_score < score_min:\n",
    "                    best_score = score_min\n",
    "                    best_state = new_state\n",
    "                    best_rejected = rejected\n",
    "\n",
    "    \n",
    "            # Update Q(s,a)\n",
    "            actions_values = get_actions_values(state, action_value_function)\n",
    "#             print(\"actions_values: \", actions_values) \n",
    "            action_value, action_index = find_value(action, actions_values, return_index=True)\n",
    "#             print(\"action_value: \", action_value, action_index)\n",
    "            # actions_values is a heap with first element being the smallest element\n",
    "            # We store values in actions_values as -Q(s,a)            \n",
    "            v = -action_value      \n",
    "#             print \"v: \", v\n",
    "            new_actions_values = get_actions_values(new_state, action_value_function)            \n",
    "#             print(\"new_actions_values: \", new_actions_values) \n",
    "            nv = -new_actions_values[0][0]\n",
    "#             print(\"nv : \", nv)\n",
    "            t = alpha * (current_reward + gamma * nv - v)\n",
    "            actions_values[action_index] = [-(v + t), action]\n",
    "#             print(\"-> actions_values: \", actions_values)\n",
    "            heapq.heapify(actions_values)\n",
    "#             print(\"--> actions_values: \", actions_values)\n",
    "                            \n",
    "            state = new_state\n",
    "            action = get_policy_action(state, action_value_function, available_gifts, epsilon=epsilon)                        \n",
    "                \n",
    "    return action_value_function, best_score, best_state, best_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_one_bag(state, action_value_function, available_gifts, ksi=0.0):\n",
    "    epsilon = 0.0\n",
    "    action = get_policy_action(state, action_value_function, available_gifts, epsilon=epsilon, ksi=0.5)\n",
    "    actions_values = get_actions_values(state, action_value_function)\n",
    "    value = find_value(action, actions_values)\n",
    "    trajectory = [(state, action, value)]\n",
    "#     print \"1 fill_one_bag: \", trajectory[-1]\n",
    "    counter = 100\n",
    "    while action is not None:\n",
    "        state = take_action(state, action)\n",
    "#         s, r = compute_score(state)\n",
    "#         if r > 10 * REJECTED_BAGS_THRESHOLD:\n",
    "#             break        \n",
    "        action = get_policy_action(state, action_value_function, available_gifts, epsilon=epsilon, ksi=ksi)\n",
    "        actions_values = get_actions_values(state, action_value_function)\n",
    "        value = find_value(action, actions_values)\n",
    "        trajectory.append((state, action, value))\n",
    "        if value >= 0:\n",
    "            break\n",
    "        \n",
    "#         print \"fill_one_bag: >>\", trajectory[-1]\n",
    "\n",
    "        counter -= 1\n",
    "        if counter == 0:\n",
    "            break\n",
    "            \n",
    "    if counter == 0:\n",
    "        logging.warn(\"Counter is zero\")\n",
    "    return trajectory[-1][0], trajectory\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n"
     ]
    }
   ],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.05\n",
    "alpha = 0.70\n",
    "goal_weight = MAX_WEIGHT * alpha\n",
    "print goal_weight\n",
    "final_action_value_function = defaultdict(list)\n",
    "#final_state = np.zeros((N_TYPES), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "available_gifts = deepcopy(AVAILABLE_GIFTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# available_gifts['ball'] = 0\n",
    "# available_gifts['bike'] = 0\n",
    "# available_gifts['blocks'] = 0\n",
    "# available_gifts['book'] = 0\n",
    "# available_gifts['coal'] = 0\n",
    "# available_gifts['doll'] = 0\n",
    "# available_gifts['gloves'] = 0\n",
    "# available_gifts['train'] = 0\n",
    "# available_gifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_state = np.zeros((N_TYPES), dtype=np.uint8)\n",
    "# final_state = np.array([2, 0, 2, 1, 0, 0, 1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# state = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "# actions_values = [[-28.171535483642572, 8], [-27.415902410504152, 7], [0, 2], [-1, 1], [0, 4], [0, 5], [0, 6], [0, 0], [0, 3]]\n",
    "# action = actions_values[0][1]   \n",
    "# new_state = take_action(state, action)\n",
    "# count = 1\n",
    "# while not is_available(new_state, available_gifts) and count < len(actions_values):\n",
    "#     action = actions_values[count][1]        \n",
    "#     new_state = take_action(state, action)\n",
    "#     print state, \"+\", action, \"=\", new_state\n",
    "#     count += 1 \n",
    "\n",
    "# print count, action, len(actions_values), is_available(new_state, available_gifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print get_policy_action(final_state, final_action_value_function, available_gifts, epsilon=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:--- Q-learning : goal=35.0, n_episodes=500\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "final_action_value_function, best_score, best_state, best_rejected = q_learning(goal_weight, \n",
    "                                                                 available_gifts,\n",
    "                                                                 initial_state=final_state,\n",
    "                                                                 n_episodes=500, \n",
    "                                                                 alpha=0.75, \n",
    "                                                                 gamma=0.95, \n",
    "                                                                 epsilon=0.25, \n",
    "                                                                 action_value_function=final_action_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.8945201744 [5 0 1 1 0 1 1 2 0] 0.0615 (38.125146883327979, 0.029999999999999999)\n"
     ]
    }
   ],
   "source": [
    "if best_state is not None:\n",
    "    print best_score, best_state, best_rejected, score((best_state,), return_rejected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_state = np.array([0, 0, 0, 0, 0, 1, 1, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5, 0, 1, 1, 0, 0, 0, 1, 1], dtype=uint8),\n",
       " (35.745715458888732,\n",
       "  10.957413932595887,\n",
       "  0.059999999999999998,\n",
       "  0.23748684174075835),\n",
       " (36.252468307648606, 0.0482))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state, score_stats((best_state,), count=200), compute_score(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 34,  14,   8, 218,   3,  26, 184,  30,  28], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIMIT_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 1 3 0 2 0 1 1] (22.27299332735203, 0.48999999999999999) (24.65849724198522, 0.41749999999999998)\n",
      "\n",
      " trajectory : [(array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 3, -32.695488152607929), (array([0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=uint8), 5, -29.15352530606987), (array([0, 0, 0, 1, 0, 1, 0, 0, 0], dtype=uint8), 7, -25.71518226097701), (array([0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=uint8), 0, -22.009117910941832), (array([1, 0, 0, 1, 0, 1, 0, 1, 0], dtype=uint8), 5, -17.94005681228637), (array([1, 0, 0, 1, 0, 2, 0, 1, 0], dtype=uint8), 8, -13.732283081054684), (array([1, 0, 0, 1, 0, 2, 0, 1, 1], dtype=uint8), 3, -9.332734374999998), (array([1, 0, 0, 2, 0, 2, 0, 1, 1], dtype=uint8), 3, -4.928125), (array([1, 0, 0, 3, 0, 2, 0, 1, 1], dtype=uint8), 0, -0.21225000000000002), (array([2, 0, 0, 3, 0, 2, 0, 1, 1], dtype=uint8), 0, 0), (array([3, 0, 0, 3, 0, 2, 0, 1, 1], dtype=uint8), 2, -1), (array([3, 0, 1, 3, 0, 2, 0, 1, 1], dtype=uint8), 1, -1)]\n"
     ]
    }
   ],
   "source": [
    "bag, trajectory = fill_one_bag(final_state, final_action_value_function, available_gifts)\n",
    "print bag, score((bag,), return_rejected=True), compute_score(bag)\n",
    "print \"\\n trajectory :\", trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_action_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --  [[-1, 4], [0, 0], [0, 1], [0, 2], [0, 3], [0, 5], [0, 6], [0, 7], [0, 8]] (32.569680167592963, 0.0045999999999999999)\n",
      "\n",
      " --  [[-1, 4], [0, 0], [0, 1], [0, 2], [0, 3], [0, 5], [0, 6], [0, 7], [0, 8]] (35.92686109957129, 0.0688)\n",
      "\n",
      " --  [[-1, 4], [0, 0], [0, 1], [0, 2], [0, 3], [0, 5], [0, 6], [0, 7], [0, 8]] (35.337153070331922, 0.1167)\n"
     ]
    }
   ],
   "source": [
    "state = np.array([2, 0, 1, 1, 0, 0, 0, 1, 1])\n",
    "print \"\\n -- \", get_actions_values(state, final_action_value_function), compute_score(state)\n",
    "state = np.array([5, 0, 1, 0, 0, 0, 2, 1, 1])\n",
    "print \"\\n -- \", get_actions_values(state, final_action_value_function), compute_score(state)\n",
    "state = np.array([6, 0, 1, 0, 0, 0, 2, 1, 1])\n",
    "print \"\\n -- \", get_actions_values(state, final_action_value_function), compute_score(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for count in [100, 200, 300]:\n",
    "#     sc = []\n",
    "#     sc2 = []\n",
    "#     for i in range(200):\n",
    "#         s, r = score((best_state,), return_rejected=True, count=count)\n",
    "#         sc.append(s)\n",
    "#         rr.append(r)\n",
    "\n",
    "#     plt.figure(figsize=(12,4))\n",
    "#     plt.subplot(131)    \n",
    "#     plt.plot(sc)\n",
    "#     plt.subplot(132)\n",
    "#     plt.plot(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value function estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# REJECTED_BAGS_THRESHOLD = 0.05\n",
    "# alpha = 0.745\n",
    "# goal_weight = MAX_WEIGHT * alpha\n",
    "# print goal_weight\n",
    "filled_bags = np.zeros((N_BAGS, N_TYPES), dtype=np.uint8)\n",
    "final_action_value_function = defaultdict(list)\n",
    "available_gifts = deepcopy(AVAILABLE_GIFTS)\n",
    "bag_index = 0\n",
    "found_goal_states = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.25\n"
     ]
    }
   ],
   "source": [
    "REJECTED_BAGS_THRESHOLD = 0.075\n",
    "alpha = 0.745\n",
    "goal_weight = MAX_WEIGHT * alpha\n",
    "print goal_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ball': 1100,\n",
       " 'bike': 500,\n",
       " 'blocks': 1000,\n",
       " 'book': 1200,\n",
       " 'coal': 166,\n",
       " 'doll': 1000,\n",
       " 'gloves': 200,\n",
       " 'horse': 1000,\n",
       " 'train': 1000}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# available_gifts['ball'] = 0\n",
    "available_gifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 0, '/', 1000)\n",
      "('- Got a result : ', 37.714767712662599, array([7, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.0269)\n",
      "('Found goal bags : ', 1, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 1, '/', 1000)\n",
      "('- Got a result : ', 37.714767712662599, array([7, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.0269)\n",
      "('Found goal bags : ', 2, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 2, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 2, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 2, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 2, '/', 1000)\n",
      "('- Got a result : ', 37.714767712662599, array([7, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.0269)\n",
      "('Found goal bags : ', 3, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 3, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 3, '/', 1000)\n",
      "('- Got a result : ', 37.714767712662599, array([7, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.0269)\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 4, '/', 1000)\n",
      "('- Got a result : ', 37.994752340646137, array([8, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.059400000000000001)\n",
      "('Found goal bags : ', 5, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 5, '/', 1000)\n",
      "('- Got a result : ', 37.714767712662599, array([7, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.0269)\n",
      "('Found goal bags : ', 6, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 6, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 6, '/', 1000)\n",
      "('- Got a result : ', 37.994752340646137, array([8, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.059400000000000001)\n",
      "('Found goal bags : ', 7, '/', 1000)\n",
      "('- Got a result : ', 37.994752340646137, array([8, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.059400000000000001)\n",
      "('Found goal bags : ', 8, '/', 1000)\n",
      "('- Got a result : ', 37.390947845770654, array([7, 0, 1, 1, 0, 1, 2, 1, 0], dtype=uint8), 0.059200000000000003)\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 9, '/', 1000)\n",
      "('- Got a result : ', 37.390947845770654, array([7, 0, 1, 1, 0, 1, 2, 1, 0], dtype=uint8), 0.059200000000000003)\n",
      "('Found goal bags : ', 10, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 10, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 10, '/', 1000)\n",
      "('- Got a result : ', 37.714767712662599, array([7, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.0269)\n",
      "('Found goal bags : ', 11, '/', 1000)\n",
      "('- Got a result : ', 37.390947845770654, array([7, 0, 1, 1, 0, 1, 2, 1, 0], dtype=uint8), 0.059200000000000003)\n",
      "('Found goal bags : ', 12, '/', 1000)\n",
      "('- Got a result : ', 37.994752340646137, array([8, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 0.059400000000000001)\n",
      "('Found goal bags : ', 13, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 13, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 13, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 13, '/', 1000)\n",
      "No best state found\n",
      "('Found goal bags : ', 13, '/', 1000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-c508f8ef7306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                                  \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                  \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                                  action_value_function=final_action_value_function)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"- Got a result : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_rejected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-244ece82bab2>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(goal_weight, available_gifts, initial_state, n_episodes, alpha, gamma, epsilon, action_value_function)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mis_terminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_terminal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"New state score, reward, new_state, action : {}, {}, {} <- {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-244ece82bab2>\u001b[0m in \u001b[0;36m_is_terminal_state\u001b[0;34m(state, goal_weight)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_current_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0m_state_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_rejected\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mREJECTED_BAGS_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0m_current_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEGATIVE_REWARD\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_WEIGHT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_state_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ad5f2a819a67>\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGIFT_WEIGHTS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_WEIGHT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrejected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_TRIALS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_TRIALS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/venv/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1848\u001b[0;31m                             out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/osboxes/Documents/TDS/venv/local/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.WARN)\n",
    "n_episodes = 500\n",
    "\n",
    "last_score_computation = -1\n",
    "limit_fails = 20\n",
    "while bag_index < N_BAGS:\n",
    "    \n",
    "    print(\"Found goal bags : \", bag_index, \"/\", N_BAGS)\n",
    "    \n",
    "    final_action_value_function, best_score, best_state, best_rejected = q_learning(goal_weight, \n",
    "                                                                 available_gifts,\n",
    "                                                                 n_episodes=n_episodes, \n",
    "                                                                 alpha=0.75, \n",
    "                                                                 gamma=0.95, \n",
    "                                                                 epsilon=0.25, \n",
    "                                                                 action_value_function=final_action_value_function)\n",
    "    if best_score > 0:\n",
    "        print(\"- Got a result : \", best_score, best_state, best_rejected)\n",
    "        update_available_gifts(available_gifts, best_state, GIFT_TYPES)\n",
    "        \n",
    "        if len(found_goal_states) == 0 or (found_goal_states[-1] != best_state).any():\n",
    "            s, r = score((best_state,), return_rejected=True)\n",
    "            found_goal_states.append(tuple(best_state.tolist()))\n",
    "\n",
    "        filled_bags[bag_index, :] = best_state\n",
    "        bag_index += 1\n",
    "        \n",
    "        limit_fails = 20\n",
    "    else:\n",
    "        print(\"No best state found\")\n",
    "        limit_fails -= 1\n",
    "        \n",
    "        \n",
    "    if bag_index > 0 and (bag_index % 20) == 0 and last_score_computation < bag_index:\n",
    "            s, r = score(filled_bags, return_rejected=True)\n",
    "            print(\">>> Current score: \", s, s * N_BAGS *1.0 / bag_index, \"rejected=\", r)\n",
    "            last_score_computation = bag_index\n",
    "\n",
    "    if bag_index > 0 and (bag_index % 30) == 0 and last_score_computation < bag_index:\n",
    "        print(\">>> Currently available gifts : \", [(k, available_gifts[k]) for k in GIFT_TYPES])\n",
    "        last_score_computation = bag_index\n",
    "        \n",
    "    if limit_fails == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37.390947845770654, 0.059200000000000003) 37.9387530866 13 (7, 0, 1, 1, 0, 1, 2, 1, 0)\n",
      "(38.460788134662451, 0.023400000000000001) 39.6641924439 12 (9, 0, 1, 0, 0, 1, 0, 1, 0)\n",
      "(38.478265580233582, 0.049799999999999997) 39.0098365262 13 (9, 0, 1, 0, 0, 1, 1, 1, 0)\n",
      "(37.801712088284724, 0.036499999999999998) 39.2722407817 12 (9, 0, 1, 0, 0, 0, 0, 2, 0)\n",
      "(37.259492895193645, 0.073499999999999996) 39.631821052 13 (8, 0, 1, 1, 0, 0, 1, 2, 0)\n",
      "(37.648917892090012, 0.054100000000000002) 38.4076010746 13 (8, 0, 1, 0, 0, 1, 2, 1, 0)\n",
      "(37.994752340646137, 0.059400000000000001) 38.7947921015 13 (8, 0, 1, 1, 0, 1, 1, 1, 0)\n",
      "(38.095119379939753, 0.068699999999999997) 40.9518453327 13 (9, 0, 1, 1, 0, 1, 0, 1, 0)\n",
      "(37.714767712662599, 0.0269) 38.1553399855 12 (7, 0, 1, 1, 0, 1, 1, 1, 0)\n",
      "(37.305573908206263, 0.053400000000000003) 38.8393961478 11 (6, 0, 1, 0, 0, 1, 1, 2, 0)\n",
      "(37.291603000194648, 0.0074999999999999997) 37.0937083909 11 (8, 0, 1, 0, 0, 1, 0, 1, 0)\n",
      "(38.052692349535612, 0.019900000000000001) 37.4145905412 12 (8, 0, 1, 0, 0, 1, 1, 1, 0)\n",
      "(37.618660803294127, 0.058500000000000003) 38.2446850372 11 (7, 0, 1, 0, 0, 1, 0, 2, 0)\n",
      "(37.250045059973374, 0.027799999999999998) 38.6075814946 10 (6, 0, 1, 0, 0, 1, 0, 2, 0)\n",
      "(38.581749387272772, 0.0591) 42.0267525182 13 (10, 0, 1, 0, 0, 1, 0, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "goal_states_set = set(found_goal_states)\n",
    "# print len(goal_states_set), goal_states_set\n",
    "for s in goal_states_set:\n",
    "    print compute_score(s), score((s,)), np.sum(s), s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "(38.095119379939753, 0.068699999999999997) 39.9020672141 13 (9, 0, 1, 1, 0, 1, 0, 1, 0)\n",
    "(37.648917892090012, 0.054100000000000002) 39.0520460559 13 (8, 0, 1, 0, 0, 1, 2, 1, 0)\n",
    "(37.652930952391415, 0.019900000000000001) 39.0795947806 13 (10, 0, 1, 1, 0, 1, 0, 0, 0)\n",
    "(36.860008345751922, 0.024) 36.9942234001 10 (5, 0, 1, 0, 0, 1, 1, 2, 0)\n",
    "(37.21313118245547, 0.0246) 38.0633084521 12 (7, 0, 1, 0, 0, 1, 2, 1, 0)\n",
    "(37.994752340646137, 0.059400000000000001) 40.2544859026 13 (8, 0, 1, 1, 0, 1, 1, 1, 0)\n",
    "(38.086447938161811, 0.031099999999999999) 38.3438299552 12 (8, 0, 1, 1, 0, 1, 0, 1, 0)\n",
    "(37.305573908206263, 0.053400000000000003) 38.6453509823 11 (6, 0, 1, 0, 0, 1, 1, 2, 0)\n",
    "(38.581749387272772, 0.0591) 39.3041167162 13 (10, 0, 1, 0, 0, 1, 0, 1, 0)\n",
    "(36.554818716494374, 0.0562) 36.9726439577 11 (5, 0, 1, 0, 0, 1, 2, 2, 0)\n",
    "(36.894183567669636, 0.053800000000000001) 37.4729994437 11 (9, 0, 1, 0, 0, 0, 0, 0, 1)\n",
    "(36.881457162385722, 0.035299999999999998) 37.3395409241 10 (5, 0, 1, 1, 0, 1, 0, 2, 0)\n",
    "(37.851066429315964, 0.016799999999999999) 37.9988054798 13 (10, 0, 1, 1, 0, 0, 0, 1, 0)\n",
    "(38.214948202843573, 0.036700000000000003) 38.9746982632 14 (10, 0, 1, 1, 0, 0, 1, 1, 0)\n",
    "(37.195781554999293, 0.066799999999999998) 37.8171737284 11 (6, 0, 1, 1, 0, 1, 0, 2, 0)\n",
    "(36.894520174436764, 0.061499999999999999) 36.1927869537 11 (5, 0, 1, 1, 0, 1, 1, 2, 0)\n",
    "(36.544902719158941, 0.075700000000000003) 38.4493464264 13 (7, 0, 1, 1, 0, 0, 2, 2, 0)\n",
    "(37.714767712662599, 0.0269) 37.9893264736 12 (7, 0, 1, 1, 0, 1, 1, 1, 0)\n",
    "(37.291603000194648, 0.0074999999999999997) 37.7662323781 11 (8, 0, 1, 0, 0, 1, 0, 1, 0)\n",
    "(38.052692349535612, 0.019900000000000001) 39.2842867375 12 (8, 0, 1, 0, 0, 1, 1, 1, 0)\n",
    "(37.618660803294127, 0.058500000000000003) 38.8013657303 11 (7, 0, 1, 0, 0, 1, 0, 2, 0)\n",
    "(37.250045059973374, 0.027799999999999998) 38.3247746685 10 (6, 0, 1, 0, 0, 1, 0, 2, 0)\n",
    "```\n",
    "\n",
    "```\n",
    "(37.390947845770654, 0.059200000000000003) 37.9387530866 13 (7, 0, 1, 1, 0, 1, 2, 1, 0)\n",
    "(38.460788134662451, 0.023400000000000001) 39.6641924439 12 (9, 0, 1, 0, 0, 1, 0, 1, 0)\n",
    "(38.478265580233582, 0.049799999999999997) 39.0098365262 13 (9, 0, 1, 0, 0, 1, 1, 1, 0)\n",
    "(37.801712088284724, 0.036499999999999998) 39.2722407817 12 (9, 0, 1, 0, 0, 0, 0, 2, 0)\n",
    "(37.259492895193645, 0.073499999999999996) 39.631821052 13 (8, 0, 1, 1, 0, 0, 1, 2, 0)\n",
    "(37.648917892090012, 0.054100000000000002) 38.4076010746 13 (8, 0, 1, 0, 0, 1, 2, 1, 0)\n",
    "(37.994752340646137, 0.059400000000000001) 38.7947921015 13 (8, 0, 1, 1, 0, 1, 1, 1, 0)\n",
    "(38.095119379939753, 0.068699999999999997) 40.9518453327 13 (9, 0, 1, 1, 0, 1, 0, 1, 0)\n",
    "(37.714767712662599, 0.0269) 38.1553399855 12 (7, 0, 1, 1, 0, 1, 1, 1, 0)\n",
    "(37.305573908206263, 0.053400000000000003) 38.8393961478 11 (6, 0, 1, 0, 0, 1, 1, 2, 0)\n",
    "(37.291603000194648, 0.0074999999999999997) 37.0937083909 11 (8, 0, 1, 0, 0, 1, 0, 1, 0)\n",
    "(38.052692349535612, 0.019900000000000001) 37.4145905412 12 (8, 0, 1, 0, 0, 1, 1, 1, 0)\n",
    "(37.618660803294127, 0.058500000000000003) 38.2446850372 11 (7, 0, 1, 0, 0, 1, 0, 2, 0)\n",
    "(37.250045059973374, 0.027799999999999998) 38.6075814946 10 (6, 0, 1, 0, 0, 1, 0, 2, 0)\n",
    "(38.581749387272772, 0.0591) 42.0267525182 13 (10, 0, 1, 0, 0, 1, 0, 1, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 4], [0, 0], [0, 1], [0, 2], [0, 3], [0, 5], [0, 6], [0, 7], [0, 8]]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array((6, 0, 1, 0, 0, 1, 1, 2, 0))\n",
    "state = final_state\n",
    "get_actions_values(state, final_action_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99610,\n",
       " {'ball': 4,\n",
       "  'bike': 500,\n",
       "  'blocks': 844,\n",
       "  'book': 1132,\n",
       "  'coal': 166,\n",
       "  'doll': 849,\n",
       "  'gloves': 55,\n",
       "  'horse': 761,\n",
       "  'train': 1000})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_action_value_function), available_gifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag filling with estimated action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 2 0 2 2 1 0] (36.178264953612086, 0.01) (34.234972589310658, 0.060900000000000003)\n",
      "\n",
      " trajectory :\n",
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), 3, -1346.9084536820153) (0.0, 0.0)\n",
      "(array([0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=uint8), 7, -1412.536308702513) (2.0347066266130103, 0.0)\n",
      "(array([0, 0, 0, 1, 0, 0, 0, 1, 0], dtype=uint8), 2, -1481.6173590083977) (7.050995555142018, 0.0)\n",
      "(array([0, 0, 1, 1, 0, 0, 0, 1, 0], dtype=uint8), 5, -1554.8010446109663) (18.756783723451115, 0.0)\n",
      "(array([0, 0, 1, 1, 0, 1, 0, 1, 0], dtype=uint8), 6, -1631.4640651382128) (23.741307041801349, 0.0)\n",
      "(array([0, 0, 1, 1, 0, 1, 1, 1, 0], dtype=uint8), 6, -26.474713005320215) (25.148673454179793, 0.0)\n",
      "(array([0, 0, 1, 1, 0, 1, 2, 1, 0], dtype=uint8), 3, -22.616009068105242) (26.550871966996812, 0.0001)\n",
      "(array([0, 0, 1, 2, 0, 1, 2, 1, 0], dtype=uint8), 0, -18.545856211286875) (28.406087850704523, 0.0033999999999999998)\n",
      "(array([1, 0, 1, 2, 0, 1, 2, 1, 0], dtype=uint8), 5, -8.4486362068486951) (30.207501980215422, 0.0071000000000000004)\n",
      "(array([1, 0, 1, 2, 0, 2, 2, 1, 0], dtype=uint8), 0, -3.807569074773788) (33.319104649365201, 0.041300000000000003)\n",
      "(array([2, 0, 1, 2, 0, 2, 2, 1, 0], dtype=uint8), 3, 1.2162304687500001) (34.234972589310658, 0.060900000000000003)\n",
      "\n",
      " --  [[1.2162304687500001, 3], [1.3507500000000001, 7], [1.8767622070312502, 0], [1.48725, 5], [1.9365153366327281, 6], [5.0024999999999995, 4], [3.3562499999999997, 2], [5.5020000000000007, 1], [6.1053749999999996, 8]]\n",
      "\n",
      " --  [[-1, 2], [0, 0], [0, 1], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8]] (38.581749387272772, 0.0591)\n",
      "\n",
      " --  [[-1346.9967272791034, 6], [-1346.9084536820153, 3], [-1346.7456249101335, 5], [-1346.8952727407204, 7], [-8.534256872770932, 4], [-48.664774502876, 8], [-1345.0315674526257, 0], [-34.249726834886445, 1], [-1346.2296098707645, 2]] (0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "final_state = np.zeros((N_TYPES), dtype=np.uint8)\n",
    "bag, trajectory = fill_one_bag(final_state, final_action_value_function, available_gifts, ksi=0.10)\n",
    "print bag, score((bag,), return_rejected=True), compute_score(bag)\n",
    "\n",
    "print \"\\n trajectory :\"\n",
    "for pt in trajectory:\n",
    "    print pt, compute_score(pt[0])\n",
    "    \n",
    "print \"\\n -- \", get_actions_values(pt[0], final_action_value_function)\n",
    "state = np.array((10, 0, 1, 0, 0, 1, 0, 1, 0))\n",
    "print \"\\n -- \", get_actions_values(state, final_action_value_function), compute_score(state)\n",
    "state = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "print \"\\n -- \", get_actions_values(state, final_action_value_function), compute_score(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
